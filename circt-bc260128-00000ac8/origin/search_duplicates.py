#!/usr/bin/env python3
import json
import subprocess
import sys
from datetime import datetime
from typing import Dict, List, Tuple

def run_gh_search(query: str) -> List[Dict]:
    """æ‰§è¡Œ gh issue searchï¼Œè¿”å›è§£æçš„ç»“æœ"""
    try:
        result = subprocess.run(
            ["gh", "issue", "list", "-R", "llvm/circt", 
             "--search", query, "--limit", "30", "--json", "number,title,body,labels,state"],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            return json.loads(result.stdout)
        else:
            print(f"âŒ Search failed: {result.stderr}", file=sys.stderr)
            return []
    except Exception as e:
        print(f"âŒ Error during search: {e}", file=sys.stderr)
        return []

def extract_keywords():
    """ä» analysis.json æå–å…³é”®è¯"""
    with open("analysis.json") as f:
        analysis = json.load(f)
    
    keywords = {
        "tool": analysis.get("tool", ""),  # arcilator
        "pass": analysis.get("pass", ""),  # InferStateProperties
        "crash_type": analysis.get("crash_type", ""),  # assertion
        "dialect": analysis.get("dialect", ""),  # arc
        "function": analysis["crash_location"].get("function", ""),  # applyEnableTransformation
        "cast_error": "cast<IntegerType>",  # ä»æ–­è¨€æ¶ˆæ¯æå–
        "struct_type": "packed struct",  # ä»å®é™…ç±»å‹æå–
        "struct_array": "struct array",
        "file": analysis["crash_location"].get("file", "").split("/")[-1],
    }
    return keywords

def calculate_similarity(issue: Dict, keywords: Dict, search_query: str) -> float:
    """è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•° (0-20)"""
    score = 0.0
    title = issue.get("title", "").lower()
    body = issue.get("body", "").lower()
    combined = f"{title} {body}"
    
    # å®Œå…¨åŒ¹é… (20åˆ†): ç›¸åŒå‡½æ•°å’Œé”™è¯¯ç±»å‹
    if (keywords["function"] in combined or 
        keywords["pass"] in combined) and keywords["crash_type"] in combined:
        score += 20.0
        return score
    
    # é«˜åº¦ç›¸å…³ (15åˆ†): åŒä¸€ passï¼Œä¸åŒé”™è¯¯
    if keywords["pass"] in combined:
        if keywords["dialect"] in combined:
            score += 15.0
            return score
        score += 12.0
    
    # ä¸­åº¦ç›¸å…³ (10åˆ†): åŒä¸€ dialect
    if keywords["dialect"] in combined:
        if "struct" in combined or "cast" in combined:
            score += 10.0
            return score
        score += 7.0
    
    # å¼±ç›¸å…³ (5åˆ†): ç›¸åŒé”™è¯¯ç±»å‹æˆ–å·¥å…·
    if keywords["tool"] in combined or keywords["crash_type"] in combined:
        score += 5.0
    
    if "packed struct" in combined or "struct array" in combined:
        score += 3.0
    
    if "IntegerType" in combined or "cast<" in combined:
        score += 2.0
    
    return min(score, 20.0)

def main():
    print("ğŸ” Extracting keywords from analysis.json...", file=sys.stderr)
    keywords = extract_keywords()
    
    print(f"Keywords: {keywords}", file=sys.stderr)
    
    # å®šä¹‰æœç´¢æŸ¥è¯¢
    searches = [
        "arcilator crash",
        "InferStateProperties assertion",
        "packed struct",
        "cast<IntegerType>",
        "struct array",
    ]
    
    all_results = {}
    issue_set = set()  # å»é‡
    
    print(f"ğŸ” Searching {len(searches)} queries...", file=sys.stderr)
    
    for query in searches:
        print(f"  â€¢ Searching: {query}", file=sys.stderr)
        results = run_gh_search(query)
        all_results[query] = results
        
        for issue in results:
            issue_num = issue["number"]
            if issue_num not in issue_set:
                issue_set.add(issue_num)
    
    # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
    scored_issues = []
    for query, issues in all_results.items():
        for issue in issues:
            # é¿å…é‡å¤è®¡åˆ†
            existing = next((x for x in scored_issues if x["number"] == issue["number"]), None)
            similarity = calculate_similarity(issue, keywords, query)
            
            if existing:
                # ä¿ç•™æœ€é«˜åˆ†
                existing["similarity"] = max(existing["similarity"], similarity)
                existing["queries"].append(query)
            else:
                scored_issues.append({
                    "number": issue["number"],
                    "title": issue.get("title", ""),
                    "state": issue.get("state", ""),
                    "labels": issue.get("labels", []),
                    "similarity": similarity,
                    "queries": [query]
                })
    
    # æ’åº
    scored_issues.sort(key=lambda x: -x["similarity"])
    
    # ç”Ÿæˆè¾“å‡º
    output = {
        "timestamp": datetime.now().isoformat(),
        "keywords": keywords,
        "total_results": len(scored_issues),
        "top_5": scored_issues[:5],
        "all_results": scored_issues,
        "search_queries": searches
    }
    
    with open("duplicates.json", "w") as f:
        json.dump(output, f, indent=2)
    
    print(json.dumps(output, indent=2))

if __name__ == "__main__":
    main()
