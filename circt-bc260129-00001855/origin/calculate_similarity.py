#!/usr/bin/env python3
import json
import re
from difflib import SequenceMatcher

# åŠ è½½æ•°æ®
with open('analysis.json', 'r') as f:
    analysis = json.load(f)

with open('search_results.json', 'r') as f:
    search_results = json.load(f)

# æå–æˆ‘ä»¬Bugçš„å…³é”®ç‰¹å¾
bug_keywords = analysis.get('keywords', [])
bug_error_msg = analysis.get('error_message', '')
bug_dialect = analysis.get('dialect', '')
bug_tool = analysis.get('tool', '')
bug_pass = analysis.get('pass_name', '')
bug_trigger = analysis.get('trigger_construct', {})

# æ„å»ºbugçš„ç‰¹å¾æ–‡æœ¬
bug_text = ' '.join([
    analysis.get('error_message', ''),
    analysis.get('pass_name', ''),
    analysis.get('tool', ''),
    analysis.get('dialect', ''),
    str(analysis.get('crash_location', {})),
    str(analysis.get('root_cause', {})),
    str(analysis.get('trigger_construct', {}))
]).lower()

print("ğŸ” Bug ç‰¹å¾æ–‡æœ¬é¢„è§ˆ:")
print(f"   å…³é”®è¯: {bug_keywords}")
print(f"   é”™è¯¯ä¿¡æ¯: {bug_error_msg}")
print(f"   æ–¹è¨€: {bug_dialect}, å·¥å…·: {bug_tool}, Pass: {bug_pass}")
print()

def calculate_similarity_score(bug_text, issue_title, issue_body):
    """è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•° (0-100)"""
    issue_text = (issue_title + ' ' + issue_body).lower()
    
    scores = {}
    
    # 1. å…³é”®è¯åŒ¹é… (40%)
    keyword_matches = sum(1 for kw in bug_keywords if kw.lower() in issue_text)
    keyword_score = min(100, (keyword_matches / len(bug_keywords)) * 100) if bug_keywords else 0
    scores['keywords'] = keyword_score
    
    # 2. é”™è¯¯æ¶ˆæ¯åŒ¹é… (30%)
    error_match = bug_error_msg.lower() in issue_text
    error_score = 100 if error_match else 0
    scores['error_message'] = error_score
    
    # 3. Dialect å’Œ Tool åŒ¹é… (20%)
    tool_match = bug_tool.lower() in issue_text
    dialect_match = bug_dialect.lower() in issue_text
    tool_score = (50 if tool_match else 0) + (50 if dialect_match else 0)
    scores['tool_dialect'] = tool_score
    
    # 4. Pass åç§°åŒ¹é… (10%)
    pass_match = bug_pass.lower() in issue_text
    pass_score = 100 if pass_match else 0
    scores['pass'] = pass_score
    
    # 5. åºåˆ—åŒ¹é… (ä½œä¸ºè¾…åŠ©å‚è€ƒ)
    seq_score = SequenceMatcher(None, bug_text[:500], issue_text[:500]).ratio() * 100
    scores['sequence'] = seq_score
    
    # åŠ æƒè®¡ç®—æ€»åˆ†
    total_score = (
        keyword_score * 0.40 +
        error_score * 0.30 +
        tool_score * 0.20 +
        pass_score * 0.10 +
        seq_score * 0.00  # åºåˆ—ä½œä¸ºå‚è€ƒä½†ä¸è®¡å…¥æ€»åˆ†
    )
    
    return round(total_score, 2), scores

# è®¡ç®—æ‰€æœ‰Issuesçš„ç›¸ä¼¼åº¦
duplicates = []

for issue in search_results['issues']:
    total_score, detail_scores = calculate_similarity_score(
        bug_text,
        issue['title'],
        issue['body']
    )
    
    duplicates.append({
        'issue_number': issue['number'],
        'title': issue['title'],
        'url': issue['url'],
        'state': issue['state'],
        'similarity_score': total_score,
        'detail_scores': detail_scores,
        'match_details': {
            'keywords_found': [kw for kw in bug_keywords if kw.lower() in (issue['title'] + ' ' + issue['body']).lower()],
            'has_error_message': bug_error_msg.lower() in (issue['title'] + ' ' + issue['body']).lower(),
            'has_tool': bug_tool.lower() in (issue['title'] + ' ' + issue['body']).lower(),
            'has_dialect': bug_dialect.lower() in (issue['title'] + ' ' + issue['body']).lower(),
            'has_pass': bug_pass.lower() in (issue['title'] + ' ' + issue['body']).lower(),
        }
    })

# æŒ‰ç›¸ä¼¼åº¦æ’åº
duplicates.sort(key=lambda x: x['similarity_score'], reverse=True)

# ç”ŸæˆæŠ¥å‘Š
report = {
    'analysis_date': '2025-02-01',
    'bug_id': analysis.get('testcase_id', ''),
    'bug_summary': {
        'dialect': bug_dialect,
        'tool': bug_tool,
        'pass': bug_pass,
        'error_message': bug_error_msg,
        'keywords': bug_keywords,
    },
    'search_summary': {
        'queries_used': len(search_results['search_queries']),
        'total_issues_found': len(search_results['issues']),
        'issues_analyzed': len(duplicates),
    },
    'duplicate_check_results': duplicates,
    'recommendation': 'pending',  # å°†åœ¨ä¸‹é¢è®¡ç®—
    'highest_similarity_score': duplicates[0]['similarity_score'] if duplicates else 0,
    'most_similar_issue': duplicates[0]['issue_number'] if duplicates else None,
}

# ç¡®å®šå»ºè®®
highest_score = report['highest_similarity_score']
if highest_score >= 80:
    report['recommendation'] = 'likely_duplicate'
    report['recommendation_reason'] = f'æ‰¾åˆ°é«˜åº¦ç›¸ä¼¼çš„Issue #{duplicates[0]["issue_number"]} (ç›¸ä¼¼åº¦: {highest_score}%)'
elif highest_score >= 50:
    report['recommendation'] = 'review_existing'
    report['recommendation_reason'] = f'æ‰¾åˆ°ä¸­ç­‰ç›¸ä¼¼åº¦çš„Issue #{duplicates[0]["issue_number"]} (ç›¸ä¼¼åº¦: {highest_score}%)ï¼Œéœ€è¦äººå·¥å®¡æŸ¥'
else:
    report['recommendation'] = 'new_issue'
    report['recommendation_reason'] = f'æœªæ‰¾åˆ°æ˜æ˜¾ç›¸å…³çš„Issue (æœ€é«˜ç›¸ä¼¼åº¦: {highest_score}%)ï¼Œå»ºè®®ä½œä¸ºæ–°Issueæäº¤'

# ä¿å­˜JSONç»“æœ
with open('duplicates.json', 'w') as f:
    json.dump(report, f, indent=2, ensure_ascii=False)

print("âœ… ç›¸ä¼¼åº¦åˆ†æå®Œæˆ")
print(f"\nğŸ“Š åˆ†æç»“æœ:")
print(f"   åˆ†æçš„Bug: {analysis.get('testcase_id', '')}")
print(f"   æœç´¢æŸ¥è¯¢æ•°: {len(search_results['search_queries'])}")
print(f"   æ‰¾åˆ°Issues: {len(duplicates)}")
print(f"   æœ€é«˜ç›¸ä¼¼åº¦: {highest_score}%")
print(f"   æœ€ç›¸ä¼¼Issue: #{report['most_similar_issue']}")
print(f"   å»ºè®®: {report['recommendation']}")
print()

# æ˜¾ç¤ºè¯¦ç»†ç»“æœ
for i, dup in enumerate(duplicates, 1):
    print(f"\n{i}. Issue #{dup['issue_number']} - ç›¸ä¼¼åº¦: {dup['similarity_score']}%")
    print(f"   æ ‡é¢˜: {dup['title']}")
    print(f"   URL: {dup['url']}")
    print(f"   åŒ¹é…å…³é”®è¯: {dup['match_details']['keywords_found']}")

